{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EECS 595 HW3: Debug SFT Training\n",
        "\n",
        "This notebook provides step-by-step verification of your SFT (Supervised Fine-Tuning) implementation.\n",
        "\n",
        "## Instructions for Students:\n",
        "\n",
        "1. **Implement the TODO sections** in `sft.py` before running the corresponding cells\n",
        "2. **Run each cell in order** to verify your implementation step by step\n",
        "3. **Use `importlib.reload()`** to reload your latest code changes\n",
        "4. **Check the output** of each test to ensure your implementation is correct\n",
        "\n",
        "## TODO Requirements by Cell:\n",
        "\n",
        "- **Cell 2**: Requires TODO 1.15 (setup_tokenizer from gpt.py)\n",
        "- **Cell 3**: Requires TODO 3.1 (SFTDataset _build_ids_labels)\n",
        "- **Cell 4**: Requires TODO 3.2 (sft_data_collator)\n",
        "- **Cell 5**: Requires TODO 3.5 (create_sft_dataloader)\n",
        "- **Cell 6**: Requires TODO 3.3 (generate_chat_response)\n",
        "- **Cell 7**: Requires TODO 3.4 (evaluate_validation_loss)\n",
        "- **Cell 8**: Requires TODO 3.1, 3.2, 3.3, 3.4, 3.5 (Complete SFT pipeline integration test)\n",
        "\n",
        "## Key Features of SFT Implementation:\n",
        "- **Conversation Formatting**: Structures dialogue data with special tokens\n",
        "- **Selective Masking**: Only trains on assistant responses, masks user/system tokens\n",
        "- **Token Masking**: Uses -100 labels to ignore certain tokens during training\n",
        "- **Conversational Generation**: Generates responses in chat-like format\n",
        "\n",
        "Let's start by importing the necessary modules and setting up the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "import importlib\n",
        "from typing import Optional, Callable, List, Tuple, Dict, Any, Iterable\n",
        "from copy import deepcopy\n",
        "import gzip\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "from torch.nn import RMSNorm\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Transformers and tokenization\n",
        "from transformers import AutoTokenizer, get_cosine_schedule_with_warmup, default_data_collator\n",
        "\n",
        "# Data handling\n",
        "from datasets import load_from_disk\n",
        "import orjson\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm.auto import tqdm, trange\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "# Import our GPT and SFT implementations\n",
        "import gpt\n",
        "import sft\n",
        "importlib.reload(gpt)  # Reload to get latest changes\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Tokenizer Setup for SFT\n",
        "\n",
        "**Required TODOs**: 1.15 (setup_tokenizer from gpt.py)\n",
        "\n",
        "First, let's set up the tokenizer with special tokens for conversational AI. This is crucial for SFT as we need special tokens to mark different parts of conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Make the tokenizer\n",
        "tokenizer = gpt.setup_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Test SFTDataset Class\n",
        "\n",
        "**Required TODOs**: 3.1 (SFTDataset _build_ids_labels)\n",
        "\n",
        "Now let's test the SFTDataset class, which is responsible for loading and formatting conversational data with proper token masking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created test file with 3 conversations\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b75c8521d79465287af5997c3f06610",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SFTDataset created successfully!\n",
            "   Dataset length: 3\n",
            "   Max length: 256\n",
            "   Sample input_ids shape: torch.Size([67])\n",
            "   Sample labels shape: torch.Size([67])\n",
            "   Training tokens: 51 (76.1%)\n",
            "   Masked tokens: 16 (23.9%)\n",
            "   First 20 tokens decoded: <|user|>Hello! How are you?<|end|><|assistant|>Hi there! I'm doing well, thank you for\n",
            "‚úÖ Test file cleaned up\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Test SFTDataset Class\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "# Create sample conversation data for testing\n",
        "sample_conversations = [\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Hi there! I'm doing well, thank you for asking. How can I help you today?\"},\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"I don't have access to real-time weather data, but I'd be happy to help you find weather information for your location!\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you help me solve 2x + 5 = 13?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Of course! Let's solve this step by step:\\n\\n1. Subtract 5 from both sides: 2x = 13 - 5 = 8\\n2. Divide both sides by 2: x = 8/2 = 4\\n\\nSo x = 4!\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Why don't scientists trust atoms? Because they make up everything! üòÑ\"}\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Create a temporary test file\n",
        "test_file = \"test_conversations.jsonl\"\n",
        "with open(test_file, 'w') as f:\n",
        "    for conv in sample_conversations:\n",
        "        f.write(json.dumps({\"messages\": conv}) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Created test file with {len(sample_conversations)} conversations\")\n",
        "\n",
        "# Test SFTDataset\n",
        "try:\n",
        "    dataset = sft.SFTDataset(test_file, tokenizer, max_length=256)\n",
        "    print(f\"‚úÖ SFTDataset created successfully!\")\n",
        "    print(f\"   Dataset length: {len(dataset)}\")\n",
        "    print(f\"   Max length: {dataset.max_length}\")\n",
        "\n",
        "    # Test getting a sample\n",
        "    sample_input_ids, sample_labels = dataset[0]\n",
        "    print(f\"   Sample input_ids shape: {sample_input_ids.shape}\")\n",
        "    print(f\"   Sample labels shape: {sample_labels.shape}\")\n",
        "\n",
        "    # Check masking logic\n",
        "    training_tokens = sum(1 for l in sample_labels if l != -100)\n",
        "    masked_tokens = sum(1 for l in sample_labels if l == -100)\n",
        "    total_tokens = len(sample_labels)\n",
        "\n",
        "    print(f\"   Training tokens: {training_tokens} ({training_tokens/total_tokens*100:.1f}%)\")\n",
        "    print(f\"   Masked tokens: {masked_tokens} ({masked_tokens/total_tokens*100:.1f}%)\")\n",
        "\n",
        "    # Decode first few tokens to verify format\n",
        "    first_tokens = sample_input_ids[:20]\n",
        "    decoded = tokenizer.decode(first_tokens, skip_special_tokens=False)\n",
        "    print(f\"   First 20 tokens decoded: {decoded}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing SFTDataset: {e}\")\n",
        "    print(\"   Make sure you've implemented the SFTDataset class in sft.py\")\n",
        "\n",
        "# Clean up test file\n",
        "os.remove(test_file)\n",
        "print(\"‚úÖ Test file cleaned up\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Test Data Collators\n",
        "\n",
        "**Required TODOs**: 3.2 (sft_data_collator)\n",
        "\n",
        "Let's test the data collators that handle batching for SFT training. We'll test both the regular collator and the HuggingFace-style collator for packed datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5311ec305970416bbafb7d1cc78a25c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing SFT Data Collator...\n",
            "‚úÖ Created batch with 3 samples\n",
            "‚úÖ Collator executed successfully!\n",
            "   Batch input_ids shape: torch.Size([3, 79])\n",
            "   Batch labels shape: torch.Size([3, 79])\n",
            "   Batch size: 3, Sequence length: 79\n",
            "   Padding tokens: 0 (0.0%)\n",
            "   Masked tokens: 113 (47.7%)\n",
            "\n",
            "Testing HuggingFace-style Collator...\n",
            "‚úÖ HF collator executed successfully!\n",
            "   Batch input_ids shape: torch.Size([2, 128])\n",
            "   Batch labels shape: torch.Size([2, 128])\n",
            "   Attention mask shape: torch.Size([2, 128])\n",
            "   Attention mask correct: True\n",
            "\n",
            "‚úÖ Test files cleaned up\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Test Data Collators\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "# Create test data for collator testing\n",
        "test_file = \"test_collator_data.jsonl\"\n",
        "with open(test_file, 'w') as f:\n",
        "    for conv in sample_conversations:\n",
        "        f.write(json.dumps({\"messages\": conv}) + \"\\n\")\n",
        "\n",
        "# Create dataset\n",
        "dataset = sft.SFTDataset(test_file, tokenizer, max_length=128)\n",
        "\n",
        "print(\"Testing SFT Data Collator...\")\n",
        "\n",
        "# Test sft_data_collator\n",
        "try:\n",
        "    # Create a batch of samples\n",
        "    batch_samples = []\n",
        "    for i in range(min(3, len(dataset))):\n",
        "        batch_samples.append(dataset[i])\n",
        "\n",
        "    print(f\"‚úÖ Created batch with {len(batch_samples)} samples\")\n",
        "\n",
        "    # Test the collator\n",
        "    collated_batch = sft.sft_data_collator(batch_samples)\n",
        "\n",
        "    print(f\"‚úÖ Collator executed successfully!\")\n",
        "    print(f\"   Batch input_ids shape: {collated_batch['input_ids'].shape}\")\n",
        "    print(f\"   Batch labels shape: {collated_batch['labels'].shape}\")\n",
        "\n",
        "    # Check that all sequences are the same length\n",
        "    batch_size, seq_length = collated_batch['input_ids'].shape\n",
        "    print(f\"   Batch size: {batch_size}, Sequence length: {seq_length}\")\n",
        "\n",
        "    # Check padding\n",
        "    pad_token_id = tokenizer.pad_token_id or 0\n",
        "    padding_tokens = (collated_batch['input_ids'] == pad_token_id).sum().item()\n",
        "    total_tokens = batch_size * seq_length\n",
        "    print(f\"   Padding tokens: {padding_tokens} ({padding_tokens/total_tokens*100:.1f}%)\")\n",
        "\n",
        "    # Check masking\n",
        "    masked_tokens = (collated_batch['labels'] == -100).sum().item()\n",
        "    print(f\"   Masked tokens: {masked_tokens} ({masked_tokens/total_tokens*100:.1f}%)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing sft_data_collator: {e}\")\n",
        "    print(\"   Make sure you've implemented the sft_data_collator function in sft.py\")\n",
        "\n",
        "print(\"\\nTesting HuggingFace-style Collator...\")\n",
        "\n",
        "# Test hf_collate (for packed datasets)\n",
        "try:\n",
        "    # Create mock packed data\n",
        "    packed_examples = [\n",
        "        {\n",
        "            \"input_ids\": [1, 2, 3, 4, 5] + [0] * 123,  # 128 total\n",
        "            \"labels\": [1, 2, 3, 4, 5] + [-100] * 123\n",
        "        },\n",
        "        {\n",
        "            \"input_ids\": [6, 7, 8, 9, 10] + [0] * 123,  # 128 total\n",
        "            \"labels\": [6, 7, 8, 9, 10] + [-100] * 123\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    hf_batch = sft.hf_collate(packed_examples)\n",
        "\n",
        "    print(f\"‚úÖ HF collator executed successfully!\")\n",
        "    print(f\"   Batch input_ids shape: {hf_batch['input_ids'].shape}\")\n",
        "    print(f\"   Batch labels shape: {hf_batch['labels'].shape}\")\n",
        "    print(f\"   Attention mask shape: {hf_batch['attention_mask'].shape}\")\n",
        "\n",
        "    # Verify attention mask\n",
        "    expected_mask = (hf_batch['input_ids'] != 0).long()\n",
        "    mask_correct = torch.equal(hf_batch['attention_mask'], expected_mask)\n",
        "    print(f\"   Attention mask correct: {mask_correct}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing hf_collate: {e}\")\n",
        "    print(\"   Make sure you've implemented the hf_collate function in sft.py\")\n",
        "\n",
        "# Clean up\n",
        "os.remove(test_file)\n",
        "print(\"\\n‚úÖ Test files cleaned up\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Test DataLoader Creation\n",
        "\n",
        "**Required TODOs**: 3.5 (create_sft_dataloader)\n",
        "\n",
        "Let's test the create_sft_dataloader function, which handles both regular and packed dataset formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Regular DataLoader Creation...\n",
            "Creating SFTDataset from test_dataloader_data.jsonl...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f723a32d6234ba5a21dfc8fcf05011f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ DataLoader created successfully for test_dataloader_data.jsonl!\n",
            "‚úÖ Regular DataLoader created successfully!\n",
            "   Number of batches: 2\n",
            "   Batch input_ids shape: torch.Size([2, 79])\n",
            "   Batch labels shape: torch.Size([2, 79])\n",
            "   Batch size: 2, Sequence length: 79\n",
            "\n",
            "Testing Packed DataLoader Creation...\n",
            "Loading packed dataset from nonexistent_packed_data.arrow...\n",
            "‚ö†Ô∏è  Packed dataset not found (expected for this test)\n",
            "   This is normal - packed datasets are created separately\n",
            "\n",
            "Testing DataLoader Iteration...\n",
            "   Batch 1: 2 samples\n",
            "   Batch 2: 1 samples\n",
            "‚úÖ DataLoader iteration successful!\n",
            "   Total batches: 2\n",
            "   Total samples: 3\n",
            "\n",
            "‚úÖ Test file cleaned up\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Test DataLoader Creation\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "# Create test data file\n",
        "test_file = \"test_dataloader_data.jsonl\"\n",
        "with open(test_file, 'w') as f:\n",
        "    for conv in sample_conversations:\n",
        "        f.write(json.dumps({\"messages\": conv}) + \"\\n\")\n",
        "\n",
        "print(\"Testing Regular DataLoader Creation...\")\n",
        "\n",
        "# Test regular dataloader (use_packed=False)\n",
        "try:\n",
        "    regular_loader = sft.create_sft_dataloader(\n",
        "        data_file=test_file,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=2,\n",
        "        max_length=128,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "        use_packed=False\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Regular DataLoader created successfully!\")\n",
        "    print(f\"   Number of batches: {len(regular_loader)}\")\n",
        "\n",
        "    # Test getting a batch\n",
        "    batch = next(iter(regular_loader))\n",
        "    print(f\"   Batch input_ids shape: {batch['input_ids'].shape}\")\n",
        "    print(f\"   Batch labels shape: {batch['labels'].shape}\")\n",
        "\n",
        "    # Check batch properties\n",
        "    batch_size, seq_length = batch['input_ids'].shape\n",
        "    print(f\"   Batch size: {batch_size}, Sequence length: {seq_length}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing regular DataLoader: {e}\")\n",
        "    print(\"   Make sure you've implemented the create_sft_dataloader function in sft.py\")\n",
        "\n",
        "print(\"\\nTesting Packed DataLoader Creation...\")\n",
        "\n",
        "# Test packed dataloader (use_packed=True)\n",
        "# Note: This will fail if no packed dataset exists, which is expected\n",
        "try:\n",
        "    packed_loader = sft.create_sft_dataloader(\n",
        "        data_file=\"nonexistent_packed_data.arrow\",  # This will fail\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=2,\n",
        "        max_length=128,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "        use_packed=True\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Packed DataLoader created successfully!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è  Packed dataset not found (expected for this test)\")\n",
        "    print(\"   This is normal - packed datasets are created separately\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing packed DataLoader: {e}\")\n",
        "\n",
        "print(\"\\nTesting DataLoader Iteration...\")\n",
        "\n",
        "# Test iterating through the regular dataloader\n",
        "try:\n",
        "    batch_count = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in regular_loader:\n",
        "        batch_count += 1\n",
        "        total_samples += batch['input_ids'].shape[0]\n",
        "\n",
        "        if batch_count <= 2:  # Only show first 2 batches\n",
        "            print(f\"   Batch {batch_count}: {batch['input_ids'].shape[0]} samples\")\n",
        "\n",
        "    print(f\"‚úÖ DataLoader iteration successful!\")\n",
        "    print(f\"   Total batches: {batch_count}\")\n",
        "    print(f\"   Total samples: {total_samples}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error iterating through DataLoader: {e}\")\n",
        "\n",
        "# Clean up\n",
        "os.remove(test_file)\n",
        "print(\"\\n‚úÖ Test file cleaned up\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Test Text Generation Functions\n",
        "\n",
        "**Required TODOs**: 3.3 (generate_chat_response)\n",
        "\n",
        "Now let's test the conversational text generation functions. We'll need a simple model for testing, so we'll create a minimal GPT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating a minimal GPT model for testing...\n",
            "‚úÖ Test model created successfully!\n",
            "   Model parameters: 112,304\n",
            "   Model moved to cpu\n",
            "\n",
            "Testing Single-Turn Chat Generation...\n",
            "User message: 'Hello! How are you?'\n",
            "‚ùå Error testing single-turn generation: index out of range in self\n",
            "   Make sure you've implemented the generate_chat_response function in sft.py\n",
            "\n",
            "Testing Multi-Turn Chat Generation...\n",
            "Conversation history:\n",
            "  user: Hi there!\n",
            "  assistant: Hello! How can I help you?\n",
            "  user: What's 2+2?\n",
            "‚ùå Error testing multi-turn generation: generate_chat_response() got an unexpected keyword argument 'context'\n",
            "   Make sure you've implemented the generate_multi_turn_response function in sft.py\n",
            "\n",
            "‚úÖ Text generation testing complete!\n",
            "Note: The generated text will be random since we're using an untrained model.\n",
            "This is expected - we're just testing that the functions work correctly.\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Test Text Generation Functions\n",
        "importlib.reload(gpt)  # Reload to get latest changes\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "print(\"Creating a minimal GPT model for testing...\")\n",
        "actual_vocab_size = 200\n",
        "# Create a very small model for testing\n",
        "model_config = {\n",
        "    \"vocab_size\": actual_vocab_size,\n",
        "    \"context_length\": 128,\n",
        "    \"emb_dim\": 64,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 2,\n",
        "    \"drop_rate\": 0.1,\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Create model\n",
        "    test_model = gpt.GPTModel(model_config)\n",
        "    print(f\"‚úÖ Test model created successfully!\")\n",
        "    print(f\"   Model parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
        "\n",
        "    # Move to CPU for testing\n",
        "    device = 'cpu'\n",
        "    test_model = test_model.to(device)\n",
        "    test_model.eval()\n",
        "\n",
        "    print(f\"   Model moved to {device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating test model: {e}\")\n",
        "    print(\"   Make sure you've implemented the GPTModel class in gpt.py\")\n",
        "    test_model = None\n",
        "\n",
        "if test_model is not None:\n",
        "    print(\"\\nTesting Single-Turn Chat Generation...\")\n",
        "\n",
        "    # Test generate_chat_response\n",
        "    try:\n",
        "        user_message = \"Hello! How are you?\"\n",
        "        print(f\"User message: '{user_message}'\")\n",
        "\n",
        "        response = sft.generate_chat_response(\n",
        "            model=test_model,\n",
        "            tokenizer=tokenizer,\n",
        "            user_message=user_message,\n",
        "            max_new_tokens=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Single-turn generation successful!\")\n",
        "        print(f\"   Generated response: '{response}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing single-turn generation: {e}\")\n",
        "        print(\"   Make sure you've implemented the generate_chat_response function in sft.py\")\n",
        "\n",
        "    print(\"\\nTesting Multi-Turn Chat Generation...\")\n",
        "\n",
        "    # Test generate_multi_turn_response\n",
        "    try:\n",
        "        conversation_history = [\n",
        "            {\"role\": \"user\", \"content\": \"Hi there!\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n",
        "            {\"role\": \"user\", \"content\": \"What's 2+2?\"}\n",
        "        ]\n",
        "\n",
        "        print(\"Conversation history:\")\n",
        "        for msg in conversation_history:\n",
        "            print(f\"  {msg['role']}: {msg['content']}\")\n",
        "\n",
        "        response = sft.generate_multi_turn_response(\n",
        "            model=test_model,\n",
        "            tokenizer=tokenizer,\n",
        "            conversation_history=conversation_history,\n",
        "            max_new_tokens=20,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Multi-turn generation successful!\")\n",
        "        print(f\"   Generated response: '{response}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing multi-turn generation: {e}\")\n",
        "        print(\"   Make sure you've implemented the generate_multi_turn_response function in sft.py\")\n",
        "\n",
        "print(\"\\n‚úÖ Text generation testing complete!\")\n",
        "print(\"Note: The generated text will be random since we're using an untrained model.\")\n",
        "print(\"This is expected - we're just testing that the functions work correctly.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Test Model Loading and Validation\n",
        "\n",
        "**Required TODOs**: 3.4 (evaluate_validation_loss)\n",
        "\n",
        "Let's test the utility functions for loading pre-trained models and evaluating validation loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Model Loading Function...\n",
            "‚úÖ Model loading error handling works: NameError\n",
            "\n",
            "Testing Validation Loss Evaluation...\n",
            "Creating SFTDataset from test_validation_data.jsonl...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0c4ac0d69394208920dd577a0e07ed8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ DataLoader created successfully for test_validation_data.jsonl!\n",
            "‚úÖ Validation DataLoader created with 2 batches\n",
            "‚ùå Error testing validation loss: name 'test_model' is not defined\n",
            "   Make sure you've implemented the evaluate_validation_loss function in sft.py\n",
            "\n",
            "‚úÖ Test files cleaned up\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Test Model Loading and Validation\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "print(\"Testing Model Loading Function...\")\n",
        "\n",
        "# Test load_pretrained_model with a non-existent file (expected to fail)\n",
        "try:\n",
        "    fake_model = sft.load_pretrained_model(\"nonexistent_model.pth\", model_config)\n",
        "    print(\"‚ùå This should have failed!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚úÖ Model loading correctly handles missing files\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úÖ Model loading error handling works: {type(e).__name__}\")\n",
        "\n",
        "print(\"\\nTesting Validation Loss Evaluation...\")\n",
        "\n",
        "# Create a small validation dataset for testing\n",
        "val_file = \"test_validation_data.jsonl\"\n",
        "with open(val_file, 'w') as f:\n",
        "    for conv in sample_conversations[:2]:  # Use only 2 conversations\n",
        "        f.write(json.dumps({\"messages\": conv}) + \"\\n\")\n",
        "\n",
        "try:\n",
        "    # Create validation dataloader\n",
        "    val_loader = sft.create_sft_dataloader(\n",
        "        data_file=val_file,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=1,\n",
        "        max_length=64,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "        use_packed=False\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Validation DataLoader created with {len(val_loader)} batches\")\n",
        "\n",
        "    # Test evaluate_validation_loss\n",
        "    if test_model is not None:\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "        val_loss = sft.evaluate_validation_loss(\n",
        "            model=test_model,\n",
        "            val_loader=val_loader,\n",
        "            loss_fn=loss_fn,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Validation loss evaluation successful!\")\n",
        "        print(f\"   Validation loss: {val_loss:.4f}\")\n",
        "        print(\"   Note: This loss is from an untrained model, so it will be high.\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Skipping validation loss test - no test model available\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing validation loss: {e}\")\n",
        "    print(\"   Make sure you've implemented the evaluate_validation_loss function in sft.py\")\n",
        "\n",
        "# Clean up\n",
        "os.remove(val_file)\n",
        "print(\"\\n‚úÖ Test files cleaned up\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Integration Test - Complete SFT Pipeline\n",
        "\n",
        "**Required TODOs**: 3.1, 3.2, 3.3, 3.4, 3.5 (Complete SFT pipeline integration test)\n",
        "\n",
        "Let's run a complete integration test that combines all the SFT components together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Integration Test - Complete SFT Pipeline\n",
        "importlib.reload(gpt)  # Reload to get latest changes\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "print(\"üöÄ Running Complete SFT Pipeline Integration Test\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create comprehensive test data\n",
        "integration_test_file = \"integration_test_data.jsonl\"\n",
        "test_conversations = [\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"How do I create a list in Python?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"You can create a list in Python using square brackets. For example: my_list = [1, 2, 3, 'hello']\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Explain neural networks\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) that process information through weighted connections.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"What's the difference between supervised and unsupervised learning?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Supervised learning uses labeled training data to learn patterns, while unsupervised learning finds patterns in data without labels.\"}\n",
        "    ]\n",
        "]\n",
        "\n",
        "with open(integration_test_file, 'w') as f:\n",
        "    for conv in test_conversations:\n",
        "        f.write(json.dumps({\"messages\": conv}) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Created test data with {len(test_conversations)} conversations\")\n",
        "\n",
        "# Step 1: Create dataset\n",
        "print(\"\\nüìä Step 1: Creating SFT Dataset\")\n",
        "try:\n",
        "    dataset = sft.SFTDataset(integration_test_file, tokenizer, max_length=128)\n",
        "    print(f\"‚úÖ Dataset created: {len(dataset)} conversations\")\n",
        "\n",
        "    # Analyze masking\n",
        "    sample_input_ids, sample_labels = dataset[0]\n",
        "    training_tokens = sum(1 for l in sample_labels if l != -100)\n",
        "    masked_tokens = sum(1 for l in sample_labels if l == -100)\n",
        "    total_tokens = len(sample_labels)\n",
        "\n",
        "    print(f\"   Training tokens: {training_tokens}/{total_tokens} ({training_tokens/total_tokens*100:.1f}%)\")\n",
        "    print(f\"   Masked tokens: {masked_tokens}/{total_tokens} ({masked_tokens/total_tokens*100:.1f}%)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Dataset creation failed: {e}\")\n",
        "    dataset = None\n",
        "\n",
        "# Step 2: Create dataloader\n",
        "print(\"\\nüîÑ Step 2: Creating DataLoader\")\n",
        "if dataset is not None:\n",
        "    try:\n",
        "        dataloader = sft.create_sft_dataloader(\n",
        "            data_file=integration_test_file,\n",
        "            tokenizer=tokenizer,\n",
        "            batch_size=2,\n",
        "            max_length=128,\n",
        "            shuffle=False,\n",
        "            drop_last=False,\n",
        "            num_workers=0,\n",
        "            use_packed=False\n",
        "        )\n",
        "        print(f\"‚úÖ DataLoader created: {len(dataloader)} batches\")\n",
        "\n",
        "        # Test batch iteration\n",
        "        batch = next(iter(dataloader))\n",
        "        print(f\"   Batch shape: {batch['input_ids'].shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå DataLoader creation failed: {e}\")\n",
        "        dataloader = None\n",
        "else:\n",
        "    dataloader = None\n",
        "\n",
        "# Step 3: Test model forward pass\n",
        "print(\"\\nüß† Step 3: Testing Model Forward Pass\")\n",
        "if dataloader is not None and test_model is not None:\n",
        "    try:\n",
        "        batch = next(iter(dataloader))\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            logits = test_model(input_ids)\n",
        "\n",
        "        print(f\"‚úÖ Forward pass successful!\")\n",
        "        print(f\"   Input shape: {input_ids.shape}\")\n",
        "        print(f\"   Logits shape: {logits.shape}\")\n",
        "        print(f\"   Labels shape: {labels.shape}\")\n",
        "\n",
        "        # Test loss computation\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        shift_logits = logits[:, :-1, :].contiguous()\n",
        "        shift_labels = labels[:, 1:].contiguous()\n",
        "        loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        print(f\"   Loss: {loss.item():.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Forward pass failed: {e}\")\n",
        "\n",
        "# Step 4: Test text generation\n",
        "print(\"\\nüí¨ Step 4: Testing Text Generation\")\n",
        "if test_model is not None:\n",
        "    try:\n",
        "        # Test single-turn generation\n",
        "        user_message = \"What is artificial intelligence?\"\n",
        "        response = sft.generate_chat_response(\n",
        "            model=test_model,\n",
        "            tokenizer=tokenizer,\n",
        "            user_message=user_message,\n",
        "            max_new_tokens=15,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Single-turn generation successful!\")\n",
        "        print(f\"   User: {user_message}\")\n",
        "        print(f\"   Assistant: {response}\")\n",
        "\n",
        "        # Test multi-turn generation\n",
        "        conv_history = [\n",
        "            {\"role\": \"user\", \"content\": \"Hello!\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n",
        "            {\"role\": \"user\", \"content\": \"Tell me about AI\"}\n",
        "        ]\n",
        "\n",
        "        multi_response = sft.generate_multi_turn_response(\n",
        "            model=test_model,\n",
        "            tokenizer=tokenizer,\n",
        "            conversation_history=conv_history,\n",
        "            max_new_tokens=15,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Multi-turn generation successful!\")\n",
        "        print(f\"   Multi-turn response: {multi_response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Text generation failed: {e}\")\n",
        "\n",
        "# Step 5: Test validation\n",
        "print(\"\\nüìà Step 5: Testing Validation\")\n",
        "if dataloader is not None and test_model is not None:\n",
        "    try:\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        val_loss = sft.evaluate_validation_loss(test_model, dataloader, loss_fn, device)\n",
        "\n",
        "        print(f\"‚úÖ Validation successful!\")\n",
        "        print(f\"   Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Validation failed: {e}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ SFT Pipeline Integration Test Complete!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if dataset is not None and dataloader is not None and test_model is not None:\n",
        "    print(\"‚úÖ All core SFT components are working correctly!\")\n",
        "    print(\"‚úÖ You're ready to implement the training loop in sft_gpt.py\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Some components need to be implemented:\")\n",
        "    if dataset is None:\n",
        "        print(\"   - SFTDataset class\")\n",
        "    if dataloader is None:\n",
        "        print(\"   - create_sft_dataloader function\")\n",
        "    if test_model is None:\n",
        "        print(\"   - GPTModel class (from gpt.py)\")\n",
        "\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Implement any missing components in sft.py\")\n",
        "print(\"2. Run the SFT training script: python sft_gpt.py\")\n",
        "print(\"3. Use the ChatWithGPT.ipynb notebook to test your trained model\")\n",
        "\n",
        "# Clean up\n",
        "os.remove(integration_test_file)\n",
        "print(\"\\n‚úÖ Test files cleaned up\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### What We've Tested\n",
        "\n",
        "This notebook has systematically tested all the core SFT components:\n",
        "\n",
        "1. **‚úÖ Tokenizer Setup**: Special tokens for conversational AI\n",
        "2. **‚úÖ SFTDataset**: Loading and formatting conversations with proper masking\n",
        "3. **‚úÖ Data Collators**: Both regular and packed dataset collation\n",
        "4. **‚úÖ DataLoader Creation**: Support for both regular and packed formats\n",
        "5. **‚úÖ Text Generation**: Single-turn and multi-turn conversation generation\n",
        "6. **‚úÖ Model Loading**: Pre-trained model loading utilities\n",
        "7. **‚úÖ Validation**: Loss evaluation on validation data\n",
        "8. **‚úÖ Integration Test**: Complete SFT pipeline verification\n",
        "\n",
        "### Key Concepts Verified\n",
        "\n",
        "- **Token Masking**: Only assistant tokens contribute to loss (labels != -100)\n",
        "- **Special Tokens**: Proper handling of `<|user|>`, `<|assistant|>`, `<|end|>`, `<|system|>`\n",
        "- **Data Formats**: Both regular jsonlines and packed Arrow datasets\n",
        "- **Conversation Format**: Proper structuring of multi-turn dialogues\n",
        "- **Generation**: Autoregressive text generation with conversation context\n",
        "\n",
        "### Implementation Checklist\n",
        "\n",
        "Before running SFT training, make sure you've implemented:\n",
        "\n",
        "- [ ] `SFTDataset` class in `sft.py`\n",
        "- [ ] `sft_data_collator` function in `sft.py`\n",
        "- [ ] `hf_collate` function in `sft.py`\n",
        "- [ ] `create_sft_dataloader` function in `sft.py`\n",
        "- [ ] `generate_chat_response` function in `sft.py`\n",
        "- [ ] `generate_multi_turn_response` function in `sft.py`\n",
        "- [ ] `load_pretrained_model` function in `sft.py`\n",
        "- [ ] `evaluate_validation_loss` function in `sft.py`\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Complete Implementation**: Implement any missing functions in `sft.py`\n",
        "2. **Run Training**: Use `python sft_gpt.py` or the provided shell scripts\n",
        "3. **Test Your Model**: Use `ChatWithGPT.ipynb` to interact with your trained model\n",
        "4. **Evaluate Performance**: Use `score_gpt.py` to test on multiple choice questions\n",
        "\n",
        "### Troubleshooting Tips\n",
        "\n",
        "- **Import Errors**: Make sure all functions are properly implemented in `sft.py`\n",
        "- **Shape Mismatches**: Check tensor dimensions at each step\n",
        "- **Masking Issues**: Verify that only assistant tokens have labels != -100\n",
        "- **Generation Problems**: Ensure special tokens are properly handled\n",
        "- **Memory Issues**: Use smaller batch sizes or max_length for testing\n",
        "\n",
        "Good luck with your SFT implementation! üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Tokenizer Setup for SFT\n",
        "importlib.reload(sft)  # Reload to get latest changes\n",
        "\n",
        "# Set up the tokenizer with special tokens for conversation\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", fast=True)\n",
        "\n",
        "# Ensure we have a pad token (GPT-2 doesn't by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
        "\n",
        "# Add conversation-specific special tokens\n",
        "special_tokens_dict = {\n",
        "    \"additional_special_tokens\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|end|>\"]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# Calculate the actual vocabulary size after adding special tokens\n",
        "special_tokens = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\", \"<|system|>\", \"<|pad|>\"]\n",
        "max_token_id = max(tokenizer.convert_tokens_to_ids(token) for token in special_tokens)\n",
        "actual_vocab_size = max_token_id + 1\n",
        "\n",
        "print(f\"‚úÖ Tokenizer initialized with {actual_vocab_size} tokens\")\n",
        "print(f\"Special token IDs:\")\n",
        "for token in special_tokens:\n",
        "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "    print(f\"  {token}: {token_id}\")\n",
        "\n",
        "# Test tokenization with conversation format\n",
        "test_conversation = \"<|user|>Hello!<|end|><|assistant|>Hi there!<|end|>\"\n",
        "tokens = tokenizer.encode(test_conversation)\n",
        "decoded = tokenizer.decode(tokens)\n",
        "\n",
        "print(f\"\\nTest conversation: '{test_conversation}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Decoded: '{decoded}'\")\n",
        "\n",
        "print(\"\\n‚úÖ Tokenizer setup complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "wereWolfLens",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
