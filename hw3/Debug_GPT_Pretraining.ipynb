{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 595 HW3: Debug GPT Pretraining\n",
    "\n",
    "This notebook provides step-by-step verification of your GPT implementation. \n",
    "\n",
    "## Instructions for Students:\n",
    "\n",
    "1. **Implement the TODO sections** in `gpt.py` before running the corresponding cells\n",
    "2. **Run each cell in order** to verify your implementation step by step\n",
    "3. **Use `importlib.reload()`** to reload your latest code changes\n",
    "4. **Check the output** of each test to ensure your implementation is correct\n",
    "\n",
    "## TODO Requirements by Cell:\n",
    "\n",
    "- **Cell 2**: Requires TODO 1.15 (setup_tokenizer)\n",
    "- **Cell 3**: Requires TODO 1.1, 1.2 (GPTEmbedding)\n",
    "- **Cell 4**: Requires TODO 1.3, 1.4 (MultiHeadAttention with RoPE)\n",
    "- **Cell 5**: Requires TODO 1.5, 1.6 (SwiGLU and FeedForward)\n",
    "- **Cell 6**: Requires TODO 1.7, 1.8, 1.9 (TransformerBlock)\n",
    "- **Cell 7**: Requires TODO 1.10, 1.11 (GPTModel)\n",
    "- **Cell 8**: Requires TODO 1.10, 1.11 (GPTModel for text generation)\n",
    "- **Cell 9**: Requires TODO 1.12, 1.13 (GPTDataset)\n",
    "- **Cell 10**: Requires TODO 1.14 (create_dataloader)\n",
    "\n",
    "## Key Features of This Implementation:\n",
    "- **RoPE (Rotary Position Embedding)**: Positional information is encoded directly in the attention mechanism\n",
    "- **No separate position embeddings**: The `GPTEmbedding` layer only handles token embeddings\n",
    "- **RoPE in attention**: Queries and keys are rotated based on their position using RoPE\n",
    "\n",
    "Let's start by importing the necessary modules and setting up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.nn import RMSNorm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# Data loading imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import bz2\n",
    "\n",
    "# Tokenization imports\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "\n",
    "# Progress and timing\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time\n",
    "\n",
    "# Import our GPT implementation\n",
    "import gpt\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Tokenizer Setup and Testing\n",
    "\n",
    "**Required TODOs**: 1.15 (setup_tokenizer)\n",
    "\n",
    "First, let's set up the tokenizer and verify it works correctly. This is important because the vocabulary size determines the size of our embedding layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Tokenizer Setup and Testing\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Set up the tokenizer\n",
    "print(\"Setting up tokenizer...\")\n",
    "tokenizer = gpt.setup_tokenizer()\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"\\nTest text: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Find special token IDs\n",
    "special_tokens = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\", \"<|system|>\", \"<|pad|>\"]\n",
    "print(f\"\\nSpecial token IDs:\")\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  {token}: {token_id}\")\n",
    "\n",
    "# Calculate actual vocabulary size needed\n",
    "max_token_id = max(tokenizer.convert_tokens_to_ids(token) for token in special_tokens)\n",
    "actual_vocab_size = max_token_id + 1\n",
    "print(f\"\\nActual vocabulary size needed: {actual_vocab_size}\")\n",
    "print(f\"Difference from tokenizer vocab size: {actual_vocab_size - tokenizer.vocab_size}\")\n",
    "\n",
    "print(\"\\n‚úÖ Tokenizer setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Test GPTEmbedding Layer (with RoPE)\n",
    "\n",
    "**Required TODOs**: 1.1 (GPTEmbedding initialization), 1.2 (GPTEmbedding forward pass)\n",
    "\n",
    "Now let's test the embedding layer. **Important**: This version only handles token embeddings - no positional embeddings since we use RoPE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test GPTEmbedding Layer\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "vocab_size = 1000\n",
    "emb_dim = 8\n",
    "context_length = 256\n",
    "batch_size = 2\n",
    "seq_length = 6\n",
    "\n",
    "# Create random token IDs\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Sample token IDs: {token_ids[0]}\")\n",
    "\n",
    "# Initialize and test the embedding layer\n",
    "print(\"\\nTesting GPTEmbedding layer...\")\n",
    "embedding_layer = gpt.GPTEmbedding(vocab_size, emb_dim, context_length)\n",
    "output = embedding_layer(token_ids)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_length, emb_dim)}\")\n",
    "print(f\"Output sample (first token): {output[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert output.shape == (batch_size, seq_length, emb_dim), \\\n",
    "    f\"Expected output shape {(batch_size, seq_length, emb_dim)}, got {output.shape}\"\n",
    "\n",
    "# Check that embeddings are different for different tokens\n",
    "if not torch.allclose(output[0, 0], output[0, 1]):\n",
    "    print(\"‚úÖ Different tokens produce different embeddings\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: Different tokens produce similar embeddings\")\n",
    "\n",
    "print(\"\\n‚úÖ GPTEmbedding layer test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Test MultiHeadAttention with RoPE\n",
    "\n",
    "**Required TODOs**: 1.3 (MultiHeadAttention initialization), 1.4 (MultiHeadAttention forward pass)\n",
    "\n",
    "This is the most complex part! The attention mechanism now uses RoPE to encode positional information directly in the queries and keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test MultiHeadAttention with RoPE\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "torch.manual_seed(123)  # For reproducible results\n",
    "d_in = 16\n",
    "d_out = d_in\n",
    "num_heads = 4\n",
    "context_length = 32\n",
    "dropout = 0.0\n",
    "batch_size = 3\n",
    "seq_len = 7\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, seq_len, d_in)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Initialize MultiHeadAttention with RoPE\n",
    "print(\"\\nTesting MultiHeadAttention with RoPE...\")\n",
    "mha = gpt.MultiHeadAttention(d_in, context_length, dropout, num_heads, qkv_bias=True)\n",
    "out = mha(x)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_len, d_out)}\")\n",
    "print(f\"Output sample: {out[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert out.shape == (batch_size, seq_len, d_out), \\\n",
    "    f\"Expected output shape {(batch_size, seq_len, d_out)}, got {out.shape}\"\n",
    "assert not torch.isnan(out).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test that RoPE is working by checking positional sensitivity\n",
    "# Create two identical sequences but at different positions\n",
    "seq1 = torch.randn(1, 2, d_in)\n",
    "seq2 = torch.randn(1, 2, d_in)\n",
    "seq2[:, 0] = seq1[:, 0]  # Make first tokens identical\n",
    "seq2[:, 1] = seq1[:, 1]  # Make second tokens identical\n",
    "\n",
    "out1 = mha(seq1)\n",
    "out2 = mha(seq2)\n",
    "\n",
    "# The outputs should be different due to RoPE encoding different positions\n",
    "if not torch.allclose(out1, out2):\n",
    "    print(\"‚úÖ RoPE is working: same tokens at different positions produce different outputs\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: RoPE might not be working correctly\")\n",
    "\n",
    "print(\"\\n‚úÖ MultiHeadAttention with RoPE test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Test SwiGLU Activation Function\n",
    "\n",
    "**Required TODOs**: 1.5 (FeedForward initialization), 1.6 (FeedForward forward pass)\n",
    "\n",
    "Let's test the SwiGLU activation function, which provides better performance than traditional ReLU or GELU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test SwiGLU Activation Function\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "dimension = 16\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "\n",
    "# Create test input\n",
    "x = torch.randn(batch_size, seq_len, dimension)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input sample: {x[0, 0, :5]}\")\n",
    "\n",
    "# Initialize and test SwiGLU\n",
    "print(\"\\nTesting SwiGLU activation...\")\n",
    "swiglu = gpt.SwiGLU(dimension)\n",
    "out = swiglu(x)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_len, dimension)}\")\n",
    "print(f\"Output sample: {out[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert out.shape == (batch_size, seq_len, dimension), \\\n",
    "    f\"Expected output shape {(batch_size, seq_len, dimension)}, got {out.shape}\"\n",
    "assert not torch.isnan(out).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test that SwiGLU is non-linear\n",
    "# Create two different inputs\n",
    "x1 = torch.randn(1, 1, dimension)\n",
    "x2 = torch.randn(1, 1, dimension)\n",
    "out1 = swiglu(x1)\n",
    "out2 = swiglu(x2)\n",
    "\n",
    "# Test linearity: SwiGLU(x1 + x2) should NOT equal SwiGLU(x1) + SwiGLU(x2)\n",
    "combined_input = x1 + x2\n",
    "combined_output = swiglu(combined_input)\n",
    "sum_outputs = out1 + out2\n",
    "\n",
    "if not torch.allclose(combined_output, sum_outputs):\n",
    "    print(\"‚úÖ SwiGLU is non-linear (as expected)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: SwiGLU appears to be linear\")\n",
    "\n",
    "print(\"\\n‚úÖ SwiGLU activation test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Test FeedForward Layer\n",
    "\n",
    "**Required TODOs**: 1.5 (FeedForward initialization), 1.6 (FeedForward forward pass)\n",
    "\n",
    "Now let's test the feed-forward network that uses SwiGLU activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test FeedForward Layer\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "emb_dim = 16\n",
    "batch_size = 10\n",
    "seq_len = 4\n",
    "\n",
    "# Create test input\n",
    "x = torch.randn(batch_size, seq_len, emb_dim)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input sample: {x[0, 0, :5]}\")\n",
    "\n",
    "# Initialize and test FeedForward\n",
    "print(\"\\nTesting FeedForward layer...\")\n",
    "ff = gpt.FeedForward(emb_dim)\n",
    "out = ff(x)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_len, emb_dim)}\")\n",
    "print(f\"Output sample: {out[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert out.shape == (batch_size, seq_len, emb_dim), \\\n",
    "    f\"Expected output shape {(batch_size, seq_len, emb_dim)}, got {out.shape}\"\n",
    "assert not torch.isnan(out).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test that FeedForward transforms the input\n",
    "if not torch.allclose(x, out):\n",
    "    print(\"‚úÖ FeedForward transforms the input (as expected)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: FeedForward doesn't seem to transform the input\")\n",
    "\n",
    "print(\"\\n‚úÖ FeedForward layer test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Test TransformerBlock\n",
    "\n",
    "**Required TODOs**: 1.7 (TransformerBlock initialization), 1.8 (TransformerBlock maybe_dropout), 1.9 (TransformerBlock forward pass)\n",
    "\n",
    "Now let's test the complete transformer block that combines attention and feed-forward layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test TransformerBlock\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Test configuration\n",
    "OG_GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "# Create test input\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, OG_GPT_CONFIG[\"emb_dim\"])\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Initialize and test TransformerBlock\n",
    "print(\"\\nTesting TransformerBlock...\")\n",
    "block = gpt.TransformerBlock(OG_GPT_CONFIG)\n",
    "output = block(x)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: {(2, 4, OG_GPT_CONFIG['emb_dim'])}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert output.shape == (2, 4, OG_GPT_CONFIG[\"emb_dim\"]), \\\n",
    "    f\"Expected output shape {(2, 4, OG_GPT_CONFIG['emb_dim'])}, got {output.shape}\"\n",
    "assert not torch.isnan(output).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test that the block transforms the input\n",
    "if not torch.allclose(x, output):\n",
    "    print(\"‚úÖ TransformerBlock transforms the input (as expected)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: TransformerBlock doesn't seem to transform the input\")\n",
    "\n",
    "print(\"\\n‚úÖ TransformerBlock test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Test Complete GPTModel\n",
    "\n",
    "**Required TODOs**: 1.10 (GPTModel initialization), 1.11 (GPTModel forward pass)\n",
    "\n",
    "Now let's test the complete GPT model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test Complete GPTModel\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Calculate vocabulary size from tokenizer\n",
    "special_tokens = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\", \"<|system|>\", \"<|pad|>\"]\n",
    "max_token_id = max(tokenizer.convert_tokens_to_ids(token) for token in special_tokens)\n",
    "actual_vocab_size = max_token_id + 1\n",
    "\n",
    "# Test configuration\n",
    "CUSTOM_GPT_CONFIG = {\n",
    "    \"vocab_size\": actual_vocab_size,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "print(f\"Using vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "# Test with real tokenized text\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "token_ids = torch.tensor(token_ids)\n",
    "print(f\"Input sentence: '{sentence}'\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Token IDs shape: {token_ids.unsqueeze(0).shape}\")\n",
    "\n",
    "# Initialize and test GPTModel\n",
    "print(\"\\nTesting GPTModel...\")\n",
    "gpt_model = gpt.GPTModel(CUSTOM_GPT_CONFIG)\n",
    "output = gpt_model(token_ids.unsqueeze(0))\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: {(1, len(token_ids), actual_vocab_size)}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert output.shape == (1, len(token_ids), actual_vocab_size), \\\n",
    "    f\"Expected output shape {(1, len(token_ids), actual_vocab_size)}, got {output.shape}\"\n",
    "assert not torch.isnan(output).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Check that logits are reasonable (not all the same)\n",
    "logits_variance = output.var()\n",
    "print(f\"Logits variance: {logits_variance:.4f}\")\n",
    "if logits_variance > 0.01:\n",
    "    print(\"‚úÖ Logits have reasonable variance\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: Logits have very low variance\")\n",
    "\n",
    "print(\"\\n‚úÖ GPTModel test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Test Text Generation\n",
    "\n",
    "**Required TODOs**: 1.10 (GPTModel initialization), 1.11 (GPTModel forward pass)\n",
    "\n",
    "Let's test text generation with our untrained model (it will be random, but we can verify the mechanics work).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Test Text Generation\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Test text generation\n",
    "start_context = \"The quick brown fox\"\n",
    "print(f\"Starting context: '{start_context}'\")\n",
    "\n",
    "# Generate text (will be random since model is untrained)\n",
    "full_text = gpt.generate_text(\n",
    "    start_context=start_context,\n",
    "    tokenizer=tokenizer,\n",
    "    model=gpt_model,\n",
    "    max_new_tokens=10,\n",
    "    context_size=CUSTOM_GPT_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Generated text: '{full_text}'\")\n",
    "print(\"\\nNote: The generated text will be random since the model is untrained.\")\n",
    "print(\"This is expected! After training, the model should generate more coherent text.\")\n",
    "\n",
    "print(\"\\n‚úÖ Text generation test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Test Dataset Creation\n",
    "\n",
    "**Required TODOs**: 1.12 (GPTDataset initialization), 1.13 (GPTDataset __getitem__)\n",
    "\n",
    "Let's test the dataset creation for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Test Dataset Creation\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Create a small test dataset\n",
    "test_docs = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Natural language processing helps computers understand human language.\"\n",
    "]\n",
    "\n",
    "print(\"Test documents:\")\n",
    "for i, doc in enumerate(test_docs):\n",
    "    print(f\"  {i+1}. {doc}\")\n",
    "\n",
    "# Test GPTDataset\n",
    "print(\"\\nTesting GPTDataset...\")\n",
    "dataset = gpt.GPTDataset(test_docs, tokenizer, max_length=10, stride=5)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Get first sample\n",
    "input_ids, labels = dataset[0]\n",
    "print(f\"First sample input shape: {input_ids.shape}\")\n",
    "print(f\"First sample labels shape: {labels.shape}\")\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# Verify causal language modeling setup\n",
    "print(\"\\nVerifying causal language modeling setup...\")\n",
    "assert input_ids.shape == labels.shape, \"Input and label shapes should match\"\n",
    "assert torch.all(input_ids[1:] == labels[:-1]), \"Labels should be input shifted by 1\"\n",
    "\n",
    "print(\"‚úÖ Labels are correctly shifted by one position\")\n",
    "\n",
    "# Test with different stride\n",
    "print(\"\\nTesting with different stride...\")\n",
    "dataset_stride = gpt.GPTDataset(test_docs, tokenizer, max_length=10, stride=10)\n",
    "print(f\"Dataset with stride=10 size: {len(dataset_stride)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset creation test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Test DataLoader Creation\n",
    "\n",
    "**Required TODOs**: 1.14 (create_dataloader)\n",
    "\n",
    "Finally, let's test the DataLoader creation for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Test DataLoader Creation\n",
    "importlib.reload(gpt)  # Reload to get latest changes\n",
    "\n",
    "# Test DataLoader creation\n",
    "print(\"Testing DataLoader creation...\")\n",
    "dataloader = gpt.create_dataloader(\n",
    "    txt=test_docs,\n",
    "    batch_size=2,\n",
    "    max_length=10,\n",
    "    stride=5,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"DataLoader created successfully!\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Test getting a batch\n",
    "print(\"\\nTesting batch retrieval...\")\n",
    "for i, (batch_input_ids, batch_labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"  Input shape: {batch_input_ids.shape}\")\n",
    "    print(f\"  Labels shape: {batch_labels.shape}\")\n",
    "    print(f\"  Input IDs: {batch_input_ids[0]}\")\n",
    "    print(f\"  Labels: {batch_labels[0]}\")\n",
    "\n",
    "    # Verify batch properties\n",
    "    assert batch_input_ids.shape == batch_labels.shape, \"Batch input and label shapes should match\"\n",
    "    assert batch_input_ids.shape[0] <= 2, \"Batch size should be <= 2\"\n",
    "\n",
    "    if i >= 1:  # Only test first 2 batches\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úÖ DataLoader creation test passed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ALL TESTS PASSED! üéâ\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYour GPT implementation is working correctly!\")\n",
    "print(\"You can now proceed to train your model using pretrain_gpt.py\")\n",
    "print(\"\\nKey points about your implementation:\")\n",
    "print(\"‚úÖ RoPE encodes positional information in attention\")\n",
    "print(\"‚úÖ No separate positional embeddings needed\")\n",
    "print(\"‚úÖ SwiGLU activation for better performance\")\n",
    "print(\"‚úÖ All components work together correctly\")\n",
    "print(\"‚úÖ Ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Test Dataset Creation\n",
    "\n",
    "Let's test the dataset creation for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wereWolfLens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
