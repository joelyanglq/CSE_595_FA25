{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interactive Generation with Your Pretrained GPT\n",
        "\n",
        "Use this notebook to load a pretrained (or SFT) GPT checkpoint from this repo and generate text.\n",
        "\n",
        "- Adjust the checkpoint path and model hyperparameters to match your training run\n",
        "- Tokenizer is set up with the special tokens used in this project\n",
        "- Generation is simple and fast; for longer/creative outputs, increase `max_new_tokens`\n",
        "\n",
        "Tip: If you fine-tuned the model (SFT), you can point `checkpoint_path` to an SFT checkpoint instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:7\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports and utility paths\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import gpt  # uses this repo's GPT implementation\n",
        "\n",
        "# Prefer GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/shared/0/projects/teaching/eecs595/models/pico-gpt/pretrained-models/gpt.1B-2-epoch.rope.model.pth\n"
          ]
        }
      ],
      "source": [
        "# Configuration: set your checkpoint path and model config here\n",
        "# NOTE: Adjust to match the model you trained.\n",
        "\n",
        "# Example default paths (edit as needed)\n",
        "# - Pretraining outputs typically saved by pretrain_gpt.py\n",
        "# - SFT outputs typically saved by sft_gpt.py\n",
        " #= \"/shared/0/projects/teaching/eecs595/models/pico-gpt/pretrained-models/\"  # <- change me\n",
        "checkpoint_path = '/shared/0/projects/teaching/eecs595/models/pico-gpt/pretrained-models/gpt.1B-2-epoch.rope.model.pth'\n",
        "\n",
        "# Model architecture used during training\n",
        "config = {\n",
        "    \"vocab_size\": None,          # will be computed from tokenizer below\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 512,\n",
        "    \"n_heads\": 8,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.0,\n",
        "    \"qkv_bias\": False,\n",
        "}\n",
        "\n",
        "# Generation defaults (you can tweak later)\n",
        "max_new_tokens = 128\n",
        "temperature = 0.8\n",
        "print(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tokenizer:\n",
            "Tokens: [15496, 11, 616, 1438, 318, 6035, 1763, 741]\n",
            "Decoded: Hello, my name is Dan Pressel\n",
            "Original vocab size: 50257\n",
            "=== Adding Special Tokens ===\n",
            "After adding special tokens:\n",
            "New vocab size: 50257\n",
            "Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>', 'additional_special_tokens': ['<|system|>', '<|user|>', '<|assistant|>', '<|end|>']}\n",
            "Tokens with special tokens: [15496, 11, 616, 1438, 318, 6035, 1763, 741]\n",
            "Decoded: Hello, my name is Dan Pressel\n",
            "Tokenizer vocab size detected: 50262\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer: use repo's setup to ensure special tokens match training\n",
        "# This mirrors `gpt.setup_tokenizer()` behavior.\n",
        "\n",
        "tokenizer = gpt.setup_tokenizer()\n",
        "\n",
        "# Compute actual vocab size that includes special tokens used during training\n",
        "special_tokens = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\", \"<|system|>\", \"<|pad|>\"]\n",
        "max_token_id = max(tokenizer.convert_tokens_to_ids(tok) for tok in special_tokens)\n",
        "actual_vocab_size = max_token_id + 1\n",
        "print(f\"Tokenizer vocab size detected: {actual_vocab_size}\")\n",
        "\n",
        "if config[\"vocab_size\"] is None:\n",
        "    config[\"vocab_size\"] = actual_vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/shared/0/projects/teaching/eecs595/models/pico-gpt/pretrained-models/gpt.1B-2-epoch.rope.model.pth\n",
            "Loaded state_dict. Missing keys: 0, Unexpected keys: 0\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Load model and checkpoint\n",
        "\n",
        "# Create model in CPU first to avoid GPU OOM on load\n",
        "model = gpt.GPTModel(config)\n",
        "\n",
        "print(checkpoint_path)\n",
        "# Load checkpoint state dict (map to CPU, then move)\n",
        "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "# Backward-compat: fix a known typo from some checkpoints\n",
        "if 'embbeding.token_embeddings.weight' in state_dict:\n",
        "    corrected = {}\n",
        "    for k, v in state_dict.items():\n",
        "        corrected[k.replace('embbeding.', 'embedding.')] = v\n",
        "    state_dict = corrected\n",
        "\n",
        "# Validate vocab size compatibility with checkpoint\n",
        "original_vocab_size = state_dict['embedding.token_embeddings.weight'].shape[0]\n",
        "if original_vocab_size != config['vocab_size']:\n",
        "    raise ValueError(f\"Vocabulary size mismatch: checkpoint={original_vocab_size}, expected={config['vocab_size']}\")\n",
        "\n",
        "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "print(f\"Loaded state_dict. Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, my name is my name, first cemeter, but I like many. His name is bintmo-sickiers.\n",
            "Norris]. A name a cat\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Simple generation helper using repo's functions\n",
        "\n",
        "def generate(prompt: str, max_new_tokens: int = None, temperature: float = None):\n",
        "    max_tokens = max_new_tokens if max_new_tokens is not None else globals().get('max_new_tokens', 128)\n",
        "    temp = temperature if temperature is not None else globals().get('temperature', 1.0)\n",
        "    with torch.no_grad():\n",
        "        model_device = next(model.parameters()).device\n",
        "        encoded = tokenizer.encode(prompt)\n",
        "        input_ids = torch.tensor(encoded, dtype=torch.long, device=model_device).unsqueeze(0)\n",
        "        out_ids = gpt.generate_new_tokens(\n",
        "            model=model,\n",
        "            idx=input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            context_size=config['context_length'],\n",
        "            temperature=temp,\n",
        "        )\n",
        "        # Move to CPU for decoding only\n",
        "        out_ids = out_ids.to('cpu')\n",
        "        return tokenizer.decode(out_ids.squeeze(0).tolist())\n",
        "\n",
        "# Quick smoke test\n",
        "print(generate(\"Hello, my name is\", max_new_tokens=32, temperature=0.9))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat-style prompting (optional)\n",
        "You can format prompts with the special tokens used in training for chat-like behavior. Example:\n",
        "\n",
        "```\n",
        "<|system|>You are a helpful assistant.<|end|>\n",
        "<|user|>Write a haiku about autumn leaves.<|end|>\n",
        "<|assistant|>\n",
        "```\n",
        "\n",
        "Then call `generate(prompt)`; the model will continue the assistant response.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
