{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook with Example Code for how to Convert Pretraining Data to Arrow\n",
        "\n",
        "**NOTE: You do not need to read, understand, or use this notebook. It is provided entirely for reference.**\n",
        "\n",
        "This notebook converts a JSONL.gz dataset to PyArrow format with **optional fixed-length sequences** for efficient GPT training.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- ‚úÖ **Optional sequence length** - set to None for variable-length sequences (no padding)\n",
        "- ‚úÖ **Memory efficient** streaming processing\n",
        "- ‚úÖ **Ready to run** from start to finish\n",
        "- ‚úÖ **HuggingFace compatible** output format\n",
        "\n",
        "## What it does\n",
        "\n",
        "1. **Streams** JSONL.gz file without loading into memory\n",
        "2. **Tokenizes** text using GPT-2 tokenizer\n",
        "3. **Creates chunks** for causal language modeling:\n",
        "   - **Fixed-length**: All sequences exactly `SEQUENCE_LENGTH` tokens\n",
        "   - **Variable-length**: Each document becomes one sequence (no padding)\n",
        "4. **Saves** as HuggingFace dataset format\n",
        "\n",
        "**Note:** _If_ you ever run this, you probably want to use the variable-length \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYPERPARAMETERS - MODIFY THESE AS NEEDED\n",
        "# =============================================================================\n",
        "\n",
        "# Fixed sequence length for all generated sequences\n",
        "# Set to None to use variable-length sequences (no padding)\n",
        "SEQUENCE_LENGTH = None  # Change this to your desired sequence length, or None for variable length\n",
        "\n",
        "# Data paths\n",
        "DATA_DIR = \"data/\"\n",
        "INPUT_FILE = \"fineweb-edu-sample-10B.jsonl.gz\"\n",
        "OUTPUT_DIR = \"fineweb-edu-sample-10B\"\n",
        "\n",
        "# Processing parameters\n",
        "BATCH_SIZE = 1000  # Documents to process at once\n",
        "MAX_CHUNKS_PER_SHARD = 10000  # Chunks per output file\n",
        "\n",
        "print(f\"üîß Configuration:\")\n",
        "if SEQUENCE_LENGTH is None:\n",
        "    print(f\"   Sequence length: Variable (no padding)\")\n",
        "else:\n",
        "    print(f\"   Sequence length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"   Input file: {DATA_DIR + INPUT_FILE}\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Max chunks per shard: {MAX_CHUNKS_PER_SHARD}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "from typing import List, Dict, Any, Iterator\n",
        "import tempfile\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"‚úÖ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add special tokens if they don't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
        "\n",
        "special_tokens_dict = {\n",
        "    \"additional_special_tokens\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|end|>\"]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "print(f\"‚úÖ Tokenizer initialized\")\n",
        "print(f\"   Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"   Special tokens: {tokenizer.special_tokens_map}\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Output directory created: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Processing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_jsonl_file(file_path: str, batch_size: int) -> Iterator[List[Dict[str, Any]]]:\n",
        "    \"\"\"Stream a JSONL.gz file in batches without loading everything into memory.\"\"\"\n",
        "    batch = []\n",
        "\n",
        "    if file_path.endswith('.gz'):\n",
        "        file_handle = gzip.open(file_path, 'rt', encoding='utf-8')\n",
        "    else:\n",
        "        file_handle = open(file_path, 'r', encoding='utf-8')\n",
        "\n",
        "    try:\n",
        "        for line_num, line in enumerate(file_handle, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                doc = json.loads(line)\n",
        "                if 'text' in doc:\n",
        "                    batch.append({\n",
        "                        'text': doc['text'],\n",
        "                        'doc_id': line_num - 1\n",
        "                    })\n",
        "\n",
        "                if len(batch) >= batch_size:\n",
        "                    yield batch\n",
        "                    batch = []\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Warning: Skipping malformed JSON at line {line_num}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if batch:\n",
        "            yield batch\n",
        "\n",
        "    finally:\n",
        "        file_handle.close()\n",
        "\n",
        "\n",
        "def tokenize_and_chunk_documents(documents: List[Dict[str, Any]],\n",
        "                                tokenizer: AutoTokenizer,\n",
        "                                sequence_length: int = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Tokenize documents and create chunks for causal language modeling.\n",
        "\n",
        "    Args:\n",
        "        documents: List of documents with 'text' and 'doc_id' fields\n",
        "        tokenizer: Tokenizer to use\n",
        "        sequence_length: Fixed length for chunks, or None for variable-length sequences\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for doc in documents:\n",
        "        text = doc['text']\n",
        "        doc_id = doc['doc_id']\n",
        "\n",
        "        if not text or not text.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to tokenize document {doc_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "        if sequence_length is None:\n",
        "            # Variable-length mode: use entire document as one sequence\n",
        "            if len(token_ids) > 1:  # Need at least 2 tokens for input/labels\n",
        "                input_chunk = token_ids[:-1]  # All except last token\n",
        "                target_chunk = token_ids[1:]   # All except first token\n",
        "\n",
        "                chunks.append({\n",
        "                    'input_ids': input_chunk,\n",
        "                    'labels': target_chunk,\n",
        "                    'doc_id': doc_id,\n",
        "                    'chunk_start': 0,\n",
        "                    'chunk_end': len(token_ids)\n",
        "                })\n",
        "        else:\n",
        "            # Fixed-length mode: create chunks of exactly sequence_length\n",
        "            if len(token_ids) < sequence_length:\n",
        "                continue\n",
        "\n",
        "            # Create non-overlapping chunks of exactly sequence_length\n",
        "            for i in range(0, len(token_ids) - sequence_length + 1, sequence_length):\n",
        "                input_chunk = token_ids[i:i + sequence_length]\n",
        "                target_chunk = token_ids[i + 1:i + sequence_length + 1]\n",
        "\n",
        "                # Only keep chunks that are exactly the right length\n",
        "                if len(input_chunk) == sequence_length and len(target_chunk) == sequence_length:\n",
        "                    chunks.append({\n",
        "                        'input_ids': input_chunk,\n",
        "                        'labels': target_chunk,\n",
        "                        'doc_id': doc_id,\n",
        "                        'chunk_start': i,\n",
        "                        'chunk_end': i + sequence_length\n",
        "                    })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def create_arrow_dataset_from_chunks(chunks: List[Dict[str, Any]],\n",
        "                                   output_path: str,\n",
        "                                   shard_index: int) -> None:\n",
        "    \"\"\"Convert chunks to PyArrow format and save as a Parquet file.\"\"\"\n",
        "    if not chunks:\n",
        "        return\n",
        "\n",
        "    # Convert to PyArrow Table\n",
        "    table_data = {\n",
        "        'input_ids': [chunk['input_ids'] for chunk in chunks],\n",
        "        'labels': [chunk['labels'] for chunk in chunks],\n",
        "        'doc_id': [chunk['doc_id'] for chunk in chunks],\n",
        "        'chunk_start': [chunk['chunk_start'] for chunk in chunks],\n",
        "        'chunk_end': [chunk['chunk_end'] for chunk in chunks]\n",
        "    }\n",
        "\n",
        "    # Create Arrow schema\n",
        "    schema = pa.schema([\n",
        "        ('input_ids', pa.list_(pa.int64())),\n",
        "        ('labels', pa.list_(pa.int64())),\n",
        "        ('doc_id', pa.int64()),\n",
        "        ('chunk_start', pa.int64()),\n",
        "        ('chunk_end', pa.int64())\n",
        "    ])\n",
        "\n",
        "    # Create Arrow table and save as Parquet\n",
        "    table = pa.table(table_data, schema=schema)\n",
        "    shard_filename = f\"shard_{shard_index:06d}.parquet\"\n",
        "    shard_path = os.path.join(output_path, shard_filename)\n",
        "\n",
        "    pq.write_table(table, shard_path, compression='snappy')\n",
        "    print(f\"‚úÖ Saved shard {shard_index} with {len(chunks)} chunks\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Core processing functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Conversion Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_dataset_to_arrow(input_path: str,\n",
        "                           output_dir: str,\n",
        "                           tokenizer: AutoTokenizer,\n",
        "                           batch_size: int,\n",
        "                           max_chunks_per_shard: int,\n",
        "                           sequence_length: int = None) -> None:\n",
        "    \"\"\"Convert the entire dataset to Arrow format with optional fixed-length sequences.\"\"\"\n",
        "    if sequence_length is None:\n",
        "        print(f\"üöÄ Starting conversion with variable-length sequences (no padding)\")\n",
        "    else:\n",
        "        print(f\"üöÄ Starting conversion with sequence length: {sequence_length}\")\n",
        "    print(f\"üìÅ Input: {input_path}\")\n",
        "    print(f\"üìÅ Output: {output_dir}\")\n",
        "\n",
        "    # Initialize counters\n",
        "    total_docs_processed = 0\n",
        "    total_chunks_created = 0\n",
        "    shard_index = 0\n",
        "    current_shard_chunks = []\n",
        "\n",
        "    # First pass: count total batches for progress bar\n",
        "    print(\"üìä Counting total batches...\")\n",
        "    total_batches = 0\n",
        "    for _ in stream_jsonl_file(input_path, batch_size):\n",
        "        total_batches += 1\n",
        "\n",
        "    print(f\"üìä Found {total_batches} batches to process\")\n",
        "\n",
        "    # Process the file in batches with progress bar\n",
        "    batch_progress = tqdm(\n",
        "        stream_jsonl_file(input_path, batch_size),\n",
        "        total=total_batches,\n",
        "        desc=\"Processing batches\",\n",
        "        unit=\"batch\"\n",
        "    )\n",
        "\n",
        "    for batch_idx, doc_batch in enumerate(batch_progress):\n",
        "        # Tokenize and chunk this batch\n",
        "        batch_chunks = tokenize_and_chunk_documents(\n",
        "            doc_batch, tokenizer, sequence_length\n",
        "        )\n",
        "\n",
        "        # Add chunks to current shard\n",
        "        current_shard_chunks.extend(batch_chunks)\n",
        "\n",
        "        # Update counters\n",
        "        total_docs_processed += len(doc_batch)\n",
        "        total_chunks_created += len(batch_chunks)\n",
        "\n",
        "        # Update progress bar description\n",
        "        batch_progress.set_postfix({\n",
        "            'docs': f\"{total_docs_processed:,}\",\n",
        "            'chunks': f\"{total_chunks_created:,}\",\n",
        "            'shards': shard_index\n",
        "        })\n",
        "\n",
        "        # Save shard when it reaches max size\n",
        "        if len(current_shard_chunks) >= max_chunks_per_shard:\n",
        "            create_arrow_dataset_from_chunks(\n",
        "                current_shard_chunks, output_dir, shard_index\n",
        "            )\n",
        "            current_shard_chunks = []\n",
        "            shard_index += 1\n",
        "\n",
        "    # Close progress bar\n",
        "    batch_progress.close()\n",
        "\n",
        "    # Save final shard\n",
        "    if current_shard_chunks:\n",
        "        create_arrow_dataset_from_chunks(\n",
        "            current_shard_chunks, output_dir, shard_index\n",
        "        )\n",
        "        shard_index += 1\n",
        "\n",
        "    # Create HuggingFace dataset from all shards\n",
        "    print(f\"\\nüîÑ Creating HuggingFace dataset from {shard_index} shards...\")\n",
        "    create_huggingface_dataset(output_dir, shard_index)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"total_documents\": total_docs_processed,\n",
        "        \"total_chunks\": total_chunks_created,\n",
        "        \"total_shards\": shard_index,\n",
        "        \"sequence_length\": sequence_length,\n",
        "        \"tokenizer_name\": \"gpt2\",\n",
        "        \"vocab_size\": tokenizer.vocab_size\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(output_dir, \"dataset_info.json\")\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüéâ Conversion completed!\")\n",
        "    print(f\"üìä Final Statistics:\")\n",
        "    print(f\"   Total documents: {total_docs_processed:,}\")\n",
        "    print(f\"   Total chunks: {total_chunks_created:,}\")\n",
        "    if sequence_length is None:\n",
        "        print(f\"   Sequence length: Variable (no padding)\")\n",
        "    else:\n",
        "        print(f\"   Sequence length: {sequence_length}\")\n",
        "    print(f\"   Total shards: {shard_index}\")\n",
        "    print(f\"\\n‚úÖ Dataset ready for loading with:\")\n",
        "    print(f\"   from datasets import load_from_disk\")\n",
        "    print(f\"   dataset = load_from_disk('{output_dir}/hf_dataset')\")\n",
        "\n",
        "\n",
        "def create_huggingface_dataset(output_dir: str, num_shards: int) -> None:\n",
        "    \"\"\"Create a HuggingFace dataset from the Parquet shards.\"\"\"\n",
        "    datasets = []\n",
        "\n",
        "    # Use tqdm for shard loading progress\n",
        "    shard_progress = tqdm(range(num_shards), desc=\"Loading shards\", unit=\"shard\")\n",
        "\n",
        "    for shard_idx in shard_progress:\n",
        "        shard_filename = f\"shard_{shard_idx:06d}.parquet\"\n",
        "        shard_path = os.path.join(output_dir, shard_filename)\n",
        "\n",
        "        if os.path.exists(shard_path):\n",
        "            shard_dataset = Dataset.from_parquet(shard_path)\n",
        "            datasets.append(shard_dataset)\n",
        "            shard_progress.set_postfix({'loaded': len(datasets)})\n",
        "\n",
        "    shard_progress.close()\n",
        "\n",
        "    if datasets:\n",
        "        print(f\"   Concatenating {len(datasets)} shards...\")\n",
        "        full_dataset = concatenate_datasets(datasets)\n",
        "\n",
        "        # Save as HuggingFace dataset format\n",
        "        hf_dataset_path = os.path.join(output_dir, \"hf_dataset\")\n",
        "        full_dataset.save_to_disk(hf_dataset_path)\n",
        "\n",
        "        print(f\"‚úÖ HuggingFace dataset saved: {hf_dataset_path}\")\n",
        "        print(f\"   Dataset size: {len(full_dataset):,} samples\")\n",
        "        print(f\"   Columns: {full_dataset.column_names}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Main conversion functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Conversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the conversion with your hyperparameters\n",
        "input_path = DATA_DIR + INPUT_FILE\n",
        "\n",
        "print(f\"üöÄ Starting conversion...\")\n",
        "if SEQUENCE_LENGTH is None:\n",
        "    print(f\"   Sequence length: Variable (no padding)\")\n",
        "else:\n",
        "    print(f\"   Sequence length: {SEQUENCE_LENGTH}\")\n",
        "print(f\"   Input file: {input_path}\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Check if input file exists\n",
        "if not os.path.exists(input_path):\n",
        "    print(f\"‚ùå Input file not found: {input_path}\")\n",
        "    print(\"Please update the DATA_DIR and INPUT_FILE variables at the top of the notebook\")\n",
        "else:\n",
        "    # Run the conversion\n",
        "    convert_dataset_to_arrow(\n",
        "        input_path=input_path,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_chunks_per_shard=MAX_CHUNKS_PER_SHARD,\n",
        "        sequence_length=SEQUENCE_LENGTH\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Test the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and test the converted dataset\n",
        "hf_dataset_path = os.path.join(OUTPUT_DIR, \"hf_dataset\")\n",
        "\n",
        "if os.path.exists(hf_dataset_path):\n",
        "    print(f\"üìä Loading dataset from: {hf_dataset_path}\")\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = Dataset.load_from_disk(hf_dataset_path)\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"   Total samples: {len(dataset):,}\")\n",
        "    print(f\"   Columns: {dataset.column_names}\")\n",
        "\n",
        "    # Test a few samples\n",
        "    print(f\"\\nüß™ Testing samples:\")\n",
        "    for i in range(min(3, len(dataset))):\n",
        "        sample = dataset[i]\n",
        "        input_ids = sample['input_ids']\n",
        "        labels = sample['labels']\n",
        "\n",
        "        print(f\"\\n   Sample {i + 1}:\")\n",
        "        print(f\"   Doc ID: {sample['doc_id']}\")\n",
        "        print(f\"   Input length: {len(input_ids)}\")\n",
        "        print(f\"   Label length: {len(labels)}\")\n",
        "        print(f\"   First 10 input tokens: {input_ids[:10]}\")\n",
        "        print(f\"   First 10 label tokens: {labels[:10]}\")\n",
        "\n",
        "        # Decode a portion of the text\n",
        "        try:\n",
        "            decoded_text = tokenizer.decode(input_ids[:50])\n",
        "            print(f\"   Decoded preview: {decoded_text[:100]}...\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Decode error: {e}\")\n",
        "\n",
        "    print(f\"\\nüéâ Dataset is ready for training!\")\n",
        "    print(f\"   Use: dataset = Dataset.load_from_disk('{hf_dataset_path}')\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Dataset not found at: {hf_dataset_path}\")\n",
        "    print(\"   Run the conversion cell above first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
